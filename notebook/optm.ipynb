{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"optm.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNIBkHL6tR2qJhJZIhzTufK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Hqp-TQt4vIBw"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/sjchoi86/upstage-basic-deeplearning/blob/main/notebook/optm.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/sjchoi86/upstage-basic-deeplearning/blob/main/notebook/optm.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View Source</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"WBr-T2RD6hma"},"source":["# Regression with Different Optimizers"]},{"cell_type":"code","metadata":{"id":"MAbu9jNhrdOL"},"source":["!pip install matplotlib==3.3.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pgdbxd616YZg"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","print (\"PyTorch version:[%s].\"%(torch.__version__))\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print (\"device:[%s].\"%(device))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6IJu5BU6qJm"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"id":"1jldyTcJ6k7X"},"source":["n_data = 10000\n","x_numpy = -3+6*np.random.rand(n_data,1)\n","y_numpy = np.exp(-(x_numpy**2))*np.cos(10*x_numpy) + 3e-2*np.random.randn(n_data,1)\n","plt.figure(figsize=(8,5))\n","plt.plot(x_numpy,y_numpy,'r.',ms=2)\n","plt.show()\n","x_torch = torch.Tensor(x_numpy).to(device)\n","y_torch = torch.Tensor(y_numpy).to(device)\n","print (\"Done.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J_AHUbKlFy1t"},"source":["### Define Model"]},{"cell_type":"code","metadata":{"id":"3tJB4a1NFAd6"},"source":["class Model(nn.Module):\n","    def __init__(self,name='mlp',xdim=1,hdims=[16,16],ydim=1):\n","        super(Model, self).__init__()\n","        self.name = name\n","        self.xdim = xdim\n","        self.hdims = hdims\n","        self.ydim = ydim\n","\n","        self.layers = []\n","        prev_hdim = self.xdim\n","        for hdim in self.hdims:\n","            self.layers.append(nn.Linear(\n","                # FILL IN HERE\n","            ))\n","            self.layers.append(nn.Tanh())  # activation\n","            prev_hdim = hdim\n","        # Final layer (without activation)\n","        self.layers.append(nn.Linear(prev_hdim,self.ydim,bias=True))\n","\n","        # Concatenate all layers \n","        self.net = nn.Sequential()\n","        for l_idx,layer in enumerate(self.layers):\n","            layer_name = \"%s_%02d\"%(type(layer).__name__.lower(),l_idx)\n","            self.net.add_module(layer_name,layer)\n","\n","        self.init_param() # initialize parameters\n","    \n","    def init_param(self):\n","        for m in self.modules():\n","            if isinstance(m,nn.Conv2d): # init conv\n","                nn.init.kaiming_normal_(m.weight)\n","                nn.init.zeros_(m.bias)\n","            elif isinstance(m,nn.Linear): # lnit dense\n","                nn.init.kaiming_normal_(m.weight)\n","                nn.init.zeros_(m.bias)\n","    \n","    def forward(self,x):\n","        return self.net(x)\n","\n","print (\"Done.\")        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-gPx2m3Fvw1"},"source":["LEARNING_RATE = 1e-2\n","# Instantiate models\n","model_sgd = Model(name='mlp_sgd',xdim=1,hdims=[64,64],ydim=1).to(device)\n","model_momentum = Model(name='mlp_momentum',xdim=1,hdims=[64,64],ydim=1).to(device)\n","model_adam = Model(name='mlp_adam',xdim=1,hdims=[64,64],ydim=1).to(device)\n","# Optimizers\n","loss = nn.MSELoss()\n","optm_sgd = optim.SGD(\n","    # FILL IN HERE\n",")\n","optm_momentum = optim.SGD(\n","    # FILL IN HERE\n",")\n","optm_adam = optim.Adam(\n","    # FILL IN HERE\n",")\n","print (\"Done.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b5OO8B6WiQ9V"},"source":["### Check Parameters"]},{"cell_type":"code","metadata":{"id":"qu66dEIxFwGc"},"source":["np.set_printoptions(precision=3)\n","n_param = 0\n","for p_idx,(param_name,param) in enumerate(model_sgd.named_parameters()):\n","    if param.requires_grad:\n","        param_numpy = param.detach().cpu().numpy() # to numpy array \n","        n_param += len(param_numpy.reshape(-1))\n","        print (\"[%d] name:[%s] shape:[%s].\"%(p_idx,param_name,param_numpy.shape))\n","        print (\"    val:%s\"%(param_numpy.reshape(-1)[:5]))\n","print (\"Total number of parameters:[%s].\"%(format(n_param,',d')))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g5PP5rRvi3c6"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"B-uu6x8DFwZ9"},"source":["MAX_ITER,BATCH_SIZE,PLOT_EVERY = 1e4,64,500\n","\n","model_sgd.init_param()\n","model_momentum.init_param()\n","model_adam.init_param()\n","\n","model_sgd.train()\n","model_momentum.train()\n","model_adam.train()\n","\n","for it in range(int(MAX_ITER)):\n","    r_idx = np.random.permutation(n_data)[:BATCH_SIZE]\n","    batch_x,batch_y = x_torch[r_idx],y_torch[r_idx]\n","    \n","    # Update with Adam\n","    y_pred_adam = model_adam.forward(batch_x)\n","    loss_adam = loss(y_pred_adam,batch_y)\n","    optm_adam.zero_grad()\n","    loss_adam.backward()\n","    optm_adam.step()\n","\n","    # Update with Momentum\n","    y_pred_momentum = model_momentum.forward(batch_x)\n","    loss_momentum = loss(y_pred_momentum,batch_y)\n","    optm_momentum.zero_grad()\n","    loss_momentum.backward()\n","    optm_momentum.step()\n","\n","    # Update with SGD\n","    y_pred_sgd = model_sgd.forward(batch_x)\n","    loss_sgd = loss(y_pred_sgd,batch_y)\n","    optm_sgd.zero_grad()\n","    loss_sgd.backward()\n","    optm_sgd.step()\n","    \n","\n","    # Plot\n","    if ((it%PLOT_EVERY)==0) or (it==0) or (it==(MAX_ITER-1)):\n","        with torch.no_grad():\n","            y_sgd_numpy = model_sgd.forward(x_torch).cpu().detach().numpy()\n","            y_momentum_numpy = model_momentum.forward(x_torch).cpu().detach().numpy()\n","            y_adam_numpy = model_adam.forward(x_torch).cpu().detach().numpy()\n","            \n","            plt.figure(figsize=(8,4))\n","            plt.plot(x_numpy,y_numpy,'r.',ms=4,label='GT')\n","            plt.plot(x_numpy,y_sgd_numpy,'g.',ms=2,label='SGD')\n","            plt.plot(x_numpy,y_momentum_numpy,'b.',ms=2,label='Momentum')\n","            plt.plot(x_numpy,y_adam_numpy,'k.',ms=2,label='ADAM')\n","            plt.title(\"[%d/%d]\"%(it,MAX_ITER),fontsize=15)\n","            plt.legend(labelcolor='linecolor',loc='upper right',fontsize=15)\n","            plt.show()\n","\n","print (\"Done.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rVf8-DZnrDdz"},"source":[""],"execution_count":null,"outputs":[]}]}