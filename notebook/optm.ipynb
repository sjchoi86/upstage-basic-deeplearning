{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hqp-TQt4vIBw"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/sjchoi86/upstage-basic-deeplearning/blob/main/notebook/optm.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/sjchoi86/upstage-basic-deeplearning/blob/main/notebook/optm.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View Source</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBr-T2RD6hma"
   },
   "source": [
    "# Regression with Different Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAbu9jNhrdOL"
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib==3.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pgdbxd616YZg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6IJu5BU6qJm"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jldyTcJ6k7X"
   },
   "outputs": [],
   "source": [
    "n_data = 10000\n",
    "x_numpy = -3+6*np.random.rand(n_data,1)\n",
    "y_numpy = np.exp(-(x_numpy**2))*np.cos(10*x_numpy) + 3e-2*np.random.randn(n_data,1)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x_numpy,y_numpy,'r.',ms=2)\n",
    "plt.show()\n",
    "x_torch = torch.Tensor(x_numpy).to(device)\n",
    "y_torch = torch.Tensor(y_numpy).to(device)\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_AHUbKlFy1t"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tJB4a1NFAd6"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,name='mlp',xdim=1,hdims=[16,16],ydim=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.name = name\n",
    "        self.xdim = xdim\n",
    "        self.hdims = hdims\n",
    "        self.ydim = ydim\n",
    "\n",
    "        self.layers = []\n",
    "        prev_hdim = self.xdim\n",
    "        for hdim in self.hdims:\n",
    "            self.layers.append(nn.Linear(\n",
    "                # FILL IN HERE\n",
    "            ))\n",
    "            self.layers.append(nn.Tanh())  # activation\n",
    "            prev_hdim = hdim\n",
    "        # Final layer (without activation)\n",
    "        self.layers.append(nn.Linear(prev_hdim,self.ydim,bias=True))\n",
    "\n",
    "        # Concatenate all layers \n",
    "        self.net = nn.Sequential()\n",
    "        for l_idx,layer in enumerate(self.layers):\n",
    "            layer_name = \"%s_%02d\"%(type(layer).__name__.lower(),l_idx)\n",
    "            self.net.add_module(layer_name,layer)\n",
    "\n",
    "        self.init_param() # initialize parameters\n",
    "    \n",
    "    def init_param(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d): # init conv\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m,nn.Linear): # lnit dense\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "print (\"Done.\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-gPx2m3Fvw1"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-2\n",
    "# Instantiate models\n",
    "model_sgd = Model(name='mlp_sgd',xdim=1,hdims=[64,64],ydim=1).to(device)\n",
    "model_momentum = Model(name='mlp_momentum',xdim=1,hdims=[64,64],ydim=1).to(device)\n",
    "model_adam = Model(name='mlp_adam',xdim=1,hdims=[64,64],ydim=1).to(device)\n",
    "# Optimizers\n",
    "loss = nn.MSELoss()\n",
    "optm_sgd = optim.SGD(\n",
    "    # FILL IN HERE\n",
    ")\n",
    "optm_momentum = optim.SGD(\n",
    "    # FILL IN HERE\n",
    ")\n",
    "optm_adam = optim.Adam(\n",
    "    # FILL IN HERE\n",
    ")\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5OO8B6WiQ9V"
   },
   "source": [
    "### Check Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qu66dEIxFwGc"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "n_param = 0\n",
    "for p_idx,(param_name,param) in enumerate(model_sgd.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        param_numpy = param.detach().cpu().numpy() # to numpy array \n",
    "        n_param += len(param_numpy.reshape(-1))\n",
    "        print (\"[%d] name:[%s] shape:[%s].\"%(p_idx,param_name,param_numpy.shape))\n",
    "        print (\"    val:%s\"%(param_numpy.reshape(-1)[:5]))\n",
    "print (\"Total number of parameters:[%s].\"%(format(n_param,',d')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5PP5rRvi3c6"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-uu6x8DFwZ9"
   },
   "outputs": [],
   "source": [
    "MAX_ITER,BATCH_SIZE,PLOT_EVERY = 1e4,64,500\n",
    "\n",
    "model_sgd.init_param()\n",
    "model_momentum.init_param()\n",
    "model_adam.init_param()\n",
    "\n",
    "model_sgd.train()\n",
    "model_momentum.train()\n",
    "model_adam.train()\n",
    "\n",
    "for it in range(int(MAX_ITER)):\n",
    "    r_idx = np.random.permutation(n_data)[:BATCH_SIZE]\n",
    "    batch_x,batch_y = x_torch[r_idx],y_torch[r_idx]\n",
    "    \n",
    "    # Update with Adam\n",
    "    y_pred_adam = model_adam.forward(batch_x)\n",
    "    loss_adam = loss(y_pred_adam,batch_y)\n",
    "    optm_adam.zero_grad()\n",
    "    loss_adam.backward()\n",
    "    optm_adam.step()\n",
    "\n",
    "    # Update with Momentum\n",
    "    y_pred_momentum = model_momentum.forward(batch_x)\n",
    "    loss_momentum = loss(y_pred_momentum,batch_y)\n",
    "    optm_momentum.zero_grad()\n",
    "    loss_momentum.backward()\n",
    "    optm_momentum.step()\n",
    "\n",
    "    # Update with SGD\n",
    "    y_pred_sgd = model_sgd.forward(batch_x)\n",
    "    loss_sgd = loss(y_pred_sgd,batch_y)\n",
    "    optm_sgd.zero_grad()\n",
    "    loss_sgd.backward()\n",
    "    optm_sgd.step()\n",
    "    \n",
    "\n",
    "    # Plot\n",
    "    if ((it%PLOT_EVERY)==0) or (it==0) or (it==(MAX_ITER-1)):\n",
    "        with torch.no_grad():\n",
    "            y_sgd_numpy = model_sgd.forward(x_torch).cpu().detach().numpy()\n",
    "            y_momentum_numpy = model_momentum.forward(x_torch).cpu().detach().numpy()\n",
    "            y_adam_numpy = model_adam.forward(x_torch).cpu().detach().numpy()\n",
    "            \n",
    "            plt.figure(figsize=(8,4))\n",
    "            plt.plot(x_numpy,y_numpy,'r.',ms=4,label='GT')\n",
    "            plt.plot(x_numpy,y_sgd_numpy,'g.',ms=2,label='SGD')\n",
    "            plt.plot(x_numpy,y_momentum_numpy,'b.',ms=2,label='Momentum')\n",
    "            plt.plot(x_numpy,y_adam_numpy,'k.',ms=2,label='ADAM')\n",
    "            plt.title(\"[%d/%d]\"%(it,MAX_ITER),fontsize=15)\n",
    "            plt.legend(labelcolor='linecolor',loc='upper right',fontsize=15)\n",
    "            plt.show()\n",
    "\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVf8-DZnrDdz"
   },
   "source": [
    "###**콘텐츠 라이선스**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNIBkHL6tR2qJhJZIhzTufK",
   "collapsed_sections": [],
   "name": "optm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
